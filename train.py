# train.py

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tokenizer import CharTokenizer
from decoder import TransformerDecoder
import time

# âœ… Hyperparametreler
batch_size = 512
seq_len = 32
d_model = 128
num_heads = 8
ff_hidden_dim = 512
num_layers = 4
dropout = 0.1
learning_rate = 1e-3
epochs = 1
max_seq_len = 256

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… Cihaz:", device)

# âœ… Veri okuma ve tokenizer
print("ğŸ“‚ Veri okunuyor...")
with open("data/ataturk.txt", "r", encoding="utf-8") as f:
    text = f.read()

tokenizer = CharTokenizer(text)
encoded = tokenizer.encode(text)
vocab_size = tokenizer.vocab_size
print("ğŸ”¤ Tokenizer hazÄ±r")
print("ğŸ“ Karakter sayÄ±sÄ±:", len(encoded))
print("ğŸ”  Vocab size:", vocab_size)

# âœ… Dataset
class CharDataset(Dataset):
    def __init__(self, data, seq_len):
        self.data = data
        self.seq_len = seq_len

    def __len__(self):
        return len(self.data) - self.seq_len

    def __getitem__(self, idx):
        x = torch.tensor(self.data[idx:idx+self.seq_len])
        y = torch.tensor(self.data[idx+1:idx+self.seq_len+1])
        return x, y

dataset = CharDataset(encoded, seq_len)
print("ğŸ“š Dataset oluÅŸturuldu. Ã–rnek sayÄ±sÄ±:", len(dataset))

if len(dataset) == 0:
    raise ValueError("âŒ Dataset boÅŸ! seq_len Ã§ok bÃ¼yÃ¼k olabilir.")

loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
print("ğŸ§ƒ DataLoader hazÄ±r")

# âœ… Model
print("ğŸ§  Model oluÅŸturuluyor...")
model = TransformerDecoder(
    vocab_size=vocab_size,
    d_model=d_model,
    max_seq_len=max_seq_len,
    num_heads=num_heads,
    ff_hidden_dim=ff_hidden_dim,
    num_layers=num_layers,
    dropout=dropout
).to(device)
print("âœ… Model hazÄ±r")

# âœ… Loss ve optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# âœ… EÄŸitim dÃ¶ngÃ¼sÃ¼
print("ğŸš€ EÄŸitime baÅŸlÄ±yoruz...\n")

try:
    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch_idx, (x, y) in enumerate(loader):
            x, y = x.to(device), y.to(device)

            optimizer.zero_grad()
            output = model(x)

            output = output.view(-1, vocab_size)
            y = y.view(-1)

            loss = criterion(output, y)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            if batch_idx % 10 == 0:
                print(f"[Epoch {epoch+1}] Batch {batch_idx} - Loss: {loss.item():.4f}")

        avg_loss = total_loss / len(loader)
        print(f"\nğŸ“ˆ Epoch {epoch+1}/{epochs} tamamlandÄ± - Ortalama Loss: {avg_loss:.4f}\n")

    # Normal kayÄ±t
    torch.save(model.state_dict(), "ataturk_model.pth")
    print("âœ… Model baÅŸarÄ±yla kaydedildi: ataturk_model.pth")

except KeyboardInterrupt:
    # CTRL+C ile kayÄ±t
    torch.save(model.state_dict(), "ataturk_model_interrupt.pth")
    print("\nğŸ›‘ EÄŸitim elle durduruldu! ğŸ’¾ Model kaydedildi: ataturk_model_interrupt.pth")
